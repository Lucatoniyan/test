{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473187a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Acquisition and Processing Systems (DaPS) (ELEC0136)    \n",
    "### Final Assignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9f4a7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "#### Task 1: Data Acquisition\n",
    "\n",
    "You will first have to acquire the necessary data for conducting your study. One essential type of\n",
    "data that you will need, are the stock prices for each company from April 2017 to April 202 1 as\n",
    "described in Section 1. Since these companies are public, the data is made available online. The\n",
    "first task is for you to search and collect this data, finding the best way to access and download\n",
    "it. A good place to look is on platforms that provide free data relating to the stock market such as\n",
    "Google Finance or Yahoo! Finance.\n",
    "\n",
    "[Optional] Providing more than one method to acquire the very same or different data, e.g. from\n",
    "a downloaded comma-separated-value file and a web API, will result in a higher score.\n",
    "\n",
    "There are many valuable sources of information for analysing the stock market. In addition to time\n",
    "series depicting the evolution of stock prices, acquire auxiliary data that is likely to be useful for\n",
    "the forecast, such as:\n",
    "\n",
    "- Social Media, e.g., Twitter: This can be used to uncover the public’s sentimental\n",
    "response to the stock market\n",
    "- Financial reports: This can help explain what kind of factors are likely to affect the stock\n",
    "market the most\n",
    "- News: This can be used to draw links between current affairs and the stock market\n",
    "- Climate data: Sometimes weather data is directly correlated to some companies’ stock\n",
    "prices and should therefore be taken into account in financial analysis\n",
    "- Others: anything that can justifiably support your analysis.\n",
    "\n",
    "Remember, you are looking for historical data, not live data.\n",
    "   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2c994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire ():\n",
    "    ##### Stock Price #####\n",
    "    import yfinance as yf\n",
    "    aal_stock = yf.Ticker(\"AAL\").history(start=\"2017-04-03\", end=\"2022-01-11\")\n",
    "    \n",
    "    ##### Income Statement #####\n",
    "    import requests\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    # query from API\n",
    "    url = f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol=AAL&outputsize=full&apikey=VA3UYIEIN6KX0M2N'\n",
    "    data = requests.get(url).text\n",
    "    # read output\n",
    "    incomestatement_json = json.loads(data)\n",
    "    \n",
    "    dates = []\n",
    "    totalRevenue = []\n",
    "    grossProfit = []\n",
    "    operatingIncome = []\n",
    "    incomeBeforeTax = []\n",
    "    ebitda = []\n",
    "    netIncome = []\n",
    "\n",
    "    for data in incomestatement_json['quarterlyReports']:\n",
    "        dates.append(str(data['fiscalDateEnding']))\n",
    "        totalRevenue.append(float(data['totalRevenue']))\n",
    "        grossProfit.append(float(data['grossProfit']))\n",
    "        operatingIncome.append(float(data['operatingIncome']))\n",
    "        incomeBeforeTax.append(float(data['incomeBeforeTax']))\n",
    "        ebitda.append(float(data['ebitda']))\n",
    "        netIncome.append(float(data['netIncome']))\n",
    "\n",
    "    # load relevant data into data frame\n",
    "    df = pd.DataFrame({'Total_Revenue': totalRevenue, 'Gross_Profit': grossProfit, 'Operating_Income': operatingIncome, \n",
    "                      'Income_Before_Tax': incomeBeforeTax, 'EBITDA': ebitda, 'Net_Income': netIncome}, index=pd.to_datetime(dates))\n",
    "    IncomeStatement_raw = df[(df.index >= '2017-03-31') &\n",
    "                       (df.index <= '2021-09-30')].sort_index(axis=0,ascending=True)\n",
    "    \n",
    "    \n",
    "    ##### Air Passengers #####\n",
    "    AirPassengers_raw = pd.read_csv('Air_Passengers_American.csv')\n",
    "    AirPassengers_raw['Date'] = pd.to_datetime(AirPassengers_raw[['Year', 'Month']].assign(DAY=1)) # change the date formate to dd-mm-yyyy\n",
    "    AirPassengers_raw = AirPassengers_raw[(AirPassengers_raw['Date']>='2017-04-01') & \n",
    "                                          (AirPassengers_raw['Date']<='2021-10-01')].drop(['Year','Month'], axis=1).set_index('Date').sort_index(axis=0,ascending=True)\n",
    "    \n",
    "    ##### Jet Fuel Price #####\n",
    "    JetFuel_Price = pd.read_csv('U.S._Gulf_Coast_Kerosene-Type_Jet_Fuel_Spot_Price_FOB.csv')\n",
    "    JetFuel_Price['Date'] = pd.to_datetime(JetFuel_Price['Date'])\n",
    "    JetFuel_Price = JetFuel_Price[(JetFuel_Price['Date'] >= '2017-04-03') & \n",
    "                                  (JetFuel_Price['Date'] <= '2021-09-30')].set_index('Date').sort_index(axis=0,ascending=True)\n",
    "    \n",
    "    \n",
    "    return aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8608a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "    \n",
    "## Task 2: Data Storage\n",
    "\n",
    "Once you have found a way to acquire the relevant data, you need to decide on how to store it.\n",
    "You should choose a format that allows an efficient read access to allow training a parametric\n",
    "model. Also, the data corpus should be such that it can be easily inspected. Data can be stored\n",
    "locally, on your computer.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911b252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price):\n",
    "    import pymongo\n",
    "    aal_stock.to_csv('StockPrices_aal.csv', index=True, header=True)\n",
    "    IncomeStatement_raw.to_csv('IncomeStatement_aal.csv', index=True, header=True)\n",
    "    \n",
    "    data_stock = aal_stock.to_dict(orient = 'records')\n",
    "    data_income_statement = IncomeStatement_raw.to_dict(orient = 'records')\n",
    "    data_air_passenger = AirPassengers_raw.to_dict(orient = 'records')\n",
    "    data_fuel = JetFuel_Price.to_dict(orient = 'records')\n",
    "    \n",
    "    client = pymongo.MongoClient(\"mongodb+srv://feiyan:252019@cluster0.tlqzz.mongodb.net/AAL_data?retryWrites=true&w=majority\")\n",
    "    db = client.AAL_data #creat a database named AAL_data\n",
    "    \n",
    "    AAL_data = db.StockPrice\n",
    "    AAL_data.insert_many(data_stock)\n",
    "    \n",
    "    AAL_data = db.IncomeStatement\n",
    "    AAL_data.insert_many(data_income_statement)\n",
    "    \n",
    "    AAL_data = db.AirPassenger\n",
    "    AAL_data.insert_many(data_air_passenger)\n",
    "    \n",
    "    AAL_data = db.JetFuelPrice\n",
    "    AAL_data.insert_many(data_fuel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc234d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-warning\">\n",
    "\n",
    "[Optional] Create a simple API to allow Al retrieving the compound of data you collected. It is enough to provide a single access point to retrieve all the data, and not implement query mechanism. The API must be accessible from the web. If you engage in this task data must be stored online.  \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d057f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(data):\n",
    "    import requests\n",
    "    import json\n",
    "    url = \"https://data.mongodb-api.com/app/data-pqzti/endpoint/data/beta/action/find\"\n",
    "    payload = json.dumps({\"collection\":data, \"database\": \"AAL_data\",\"dataSource\": \"Cluster0\"})\n",
    "    headers = {'Content-Type': 'application/json','Access-Control-Request-Headers': '*',\n",
    "               'api-key': '2QI1TfX8m0dI7aYf6E5Tp9vVbzPWAq0LoS2v9VavpG0eHiQjsXvXMVF41UjjgxCU'}\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload).text\n",
    "    parsed = json.loads(response)\n",
    "    \n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac8a37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "## Task 3: Data Preprocessing\n",
    "\n",
    "Now that you have the data stored, you can start preprocessing it. Think about what features to\n",
    "keep, which ones to transform, combine or discard. Make sure your data is clean and consistent\n",
    "(e.g., are there many outliers? any missing values?). You are expected to:\n",
    "\n",
    "1. Clean the data from missing values and outliers, if any.\n",
    "2. Provide useful visualisation of the data. Plots should be saved on disk, and not printed on\n",
    "the juptyer notebook.\n",
    "3. Transform your data (e.g., using normalization, dimensionality reduction, etc.) to improve\n",
    "the forecasting performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed87f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "######### 3.1 Data Cleaning ###################################################################################################\n",
    "    \n",
    "    ##### 3.1.1 Resample #####\n",
    "    def resample(df, upsample):\n",
    "        # Upsample data.\n",
    "        df_res = df.resample(upsample)\n",
    "        # Interpolate through time\n",
    "        df_res = df_res.interpolate(method='time')\n",
    "        # Downsample to a daily basis.\n",
    "        df_res = df_res.resample('D')\n",
    "        df_res = df_res.interpolate()\n",
    "\n",
    "        df_res = df_res[(df_res.index >= '2017-04-03') &\n",
    "                        (df_res.index <= '2021-09-30')].sort_index(axis=0,ascending=True)\n",
    "        return df_res\n",
    "    \n",
    "    Stock_daily = resample(aal_stock, 'D')\n",
    "    IncomeStatement_daily = resample(IncomeStatement_raw, 'Q')\n",
    "    AirPassengers_daily = resample(AirPassengers_raw, 'D')\n",
    "    JetFuelPrice_daily = resample(JetFuel_Price, 'D')\n",
    "    \n",
    "    ##### 3.1.2 Detect Missing Value #####\n",
    "    def missing(data, label):\n",
    "        missing_val = data.isnull().sum()\n",
    "        print('Number of missing values detected in %s is \\n %s' %(label, missing_val))\n",
    "        \n",
    "    missing(Stock_daily, 'Stock Price')\n",
    "    missing(IncomeStatement_daily, 'Income Statement')\n",
    "    missing(AirPassengers_daily, 'Air Passengers')\n",
    "    missing(JetFuelPrice_daily, 'Jet Fuel Price')\n",
    "    \n",
    "    ##### 3.1.3 Find the Outliers#####\n",
    "    \n",
    "    def outliers_IQR(data, column, label):\n",
    "        dataset = np.array(column)\n",
    "        date = np.array(data.index)\n",
    "        Q1, Q3 = np.percentile(dataset, [25, 75])\n",
    "        IQR = Q3 - Q1\n",
    "        LB = Q1 - (IQR * 1.5) #Lower bound\n",
    "        UB = Q3 + (IQR * 1.5) #Upper bound\n",
    "        outlier_locs = np.where((dataset > UB) | (dataset < LB))\n",
    "        outlier_date = date[outlier_locs]\n",
    "        outlier_values = dataset[outlier_locs] \n",
    "\n",
    "        if (len(outlier_locs[0])) == 0:\n",
    "            print('%s has no outliers.' %(label))\n",
    "        else:\n",
    "            print('Outliers exist in %s is: %s' %(label, outlier_date))\n",
    "            print('The corresponding outliers are: ',outlier_values)\n",
    "    \n",
    "    outliers_IQR(Stock_daily, Stock_daily.Open, 'The open stock price')\n",
    "    outliers_IQR(Stock_daily, Stock_daily.High, 'The high stock price')\n",
    "    outliers_IQR(Stock_daily, Stock_daily.Low, 'The low stock price')\n",
    "    outliers_IQR(Stock_daily, Stock_daily.Close, 'The close stock price')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.Total_Revenue, 'The total Revenue')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.Gross_Profit, 'The gross profit')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.Operating_Income, 'The operating income')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.Income_Before_Tax, 'The income before tax')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.EBITDA, 'The ebitda')\n",
    "    outliers_IQR(IncomeStatement_daily, IncomeStatement_daily.Net_Income, 'The net income')\n",
    "    outliers_IQR(AirPassengers_daily, AirPassengers_daily.TOTAL, 'The air passengers')\n",
    "    outliers_IQR(JetFuelPrice_daily, JetFuelPrice_daily.Jet_Fuel_Price, 'jet fuel price')\n",
    "    \n",
    "    ##### 3.1.4 Cap the outliers #####\n",
    "    def outliers(data, column):\n",
    "        dataset = np.array(column)\n",
    "        date = np.array(data.index)\n",
    "        Q1, Q3 = np.percentile(dataset, [25, 75])\n",
    "        IQR = Q3 - Q1\n",
    "        LB = Q1 - (IQR * 1.5) #Lower bound\n",
    "        UB = Q3 + (IQR * 1.5) #Upper bound\n",
    "        outlier_locs = np.where((dataset > UB) | (dataset < LB))\n",
    "\n",
    "        outlier_high = np.where(dataset > UB)\n",
    "        outlier_low = np.where(dataset < LB)\n",
    "        outlier_date = date[outlier_locs]\n",
    "        outlier_values = dataset[outlier_locs] \n",
    "        # drop the outliers first\n",
    "        dropped_outlier_dataset = np.copy(data)\n",
    "        dropped_outlier_dataset = np.delete(dropped_outlier_dataset, outlier_locs)\n",
    "        # cap the outliers\n",
    "        capped_outlier_dataset=np.copy(data)\n",
    "        if ((len(outlier_high[0])!=0) & (len(outlier_low[0])==0)):\n",
    "            capped_outlier_dataset[outlier_locs]=UB\n",
    "        elif ((len(outlier_low[0])!=0) & (len(outlier_high[0])==0)):\n",
    "            capped_outlier_dataset[outlier_locs]=LB\n",
    "        elif ((len(outlier_low[0])!=0) & (len(outlier_high[0])!=0)):\n",
    "            capped_high_dataset=np.copy(data)\n",
    "            capped_high_dataset[outlier_high]=UB\n",
    "            capped_low_dataset=np.copy(data)\n",
    "            capped_low_dataset[outlier_low]=LB\n",
    "            capped_outlier_dataset[outlier_locs] = capped_high_dataset[outlier_high] + capped_low_dataset[outlier_low]\n",
    "        return outlier_locs, outlier_values, capped_outlier_dataset\n",
    "\n",
    "    def outliers_capping(data, column, name):\n",
    "        capped = pd.DataFrame(index = data.index)\n",
    "        capped[name] = outliers(data, column)[2]\n",
    "        return capped\n",
    "    \n",
    "    JetFuelPrice_Capped = outliers_capping(JetFuelPrice_daily, JetFuelPrice_daily.Jet_Fuel_Price, 'JetFuelPrice_Capped')\n",
    "    \n",
    "######### 3.2 Data Visualisation ##############################################################################################\n",
    "    \n",
    "    def data_visualisation (data, col, title, ylabel, xlabel, figname):\n",
    "        plt.figure(figsize=(16, 7))\n",
    "        \n",
    "        for column in col[0:]:\n",
    "            plt.plot(data.index, data[column], label=column)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.savefig(figname)\n",
    "        plt.close()\n",
    "    \n",
    "    # Stock Price\n",
    "    col = ['Open', 'High', 'Low', 'Close']\n",
    "    title = 'Stock Prices of American Airlines Group Inc.'\n",
    "    ylabel = r'Stock Price ($)'\n",
    "    xlabel = 'Date'\n",
    "    figname = 'aal_stock_prices.png'\n",
    "    StockPrice_visualisation = data_visualisation(aal_stock, col, title, ylabel, xlabel, figname)\n",
    "    \n",
    "    # Income Statement\n",
    "    col = IncomeStatement_daily.columns\n",
    "    title = 'Income Statement Indices of American Airlines Group Inc.'\n",
    "    ylabel = 'Dollar'\n",
    "    xlabel = 'Date'\n",
    "    figname = 'income_statement.png'\n",
    "    IncomeStatement_visualisation = data_visualisation(IncomeStatement_daily, col, title, ylabel, xlabel, figname)\n",
    "    \n",
    "    # Air Passenger\n",
    "    col = AirPassengers_daily.columns\n",
    "    title = 'Air Passenger number of the United States of America'\n",
    "    ylabel = 'Number of Passengers'\n",
    "    xlabel = 'Date'\n",
    "    figname = 'air_passengers.png'\n",
    "    AirPassenger_visualisation = data_visualisation(AirPassengers_daily, col, title, ylabel, xlabel, figname)\n",
    "    \n",
    "    # Jet Fuel Price\n",
    "    col = JetFuelPrice_daily.columns\n",
    "    title = 'Income Statement Indices of American Airlines Group Inc.'\n",
    "    ylabel = r'Jet Fuel Price ($)'\n",
    "    xlabel = 'Date'\n",
    "    figname = 'jet_fuel_price.png'\n",
    "    JetFuelPrice_visualisation = data_visualisation(JetFuelPrice_daily, col, title, ylabel, xlabel, figname)\n",
    "    \n",
    "    def outliers_visualisation(data, title, column, label):    \n",
    "        sns.boxplot(x = column)\n",
    "        plt.title('The IQR Score Boxplot of %s'%(title))\n",
    "        plt.xlabel(label)\n",
    "        plt.savefig('outliers_boxplot.png')\n",
    "        plt.close()\n",
    "\n",
    "        outlier_locs = outliers(data, column)[0]\n",
    "        outlier_values = outliers(data, column)[1]\n",
    "        capped_outlier_dataset = outliers(data, column)[2]\n",
    "\n",
    "        # plot and compare them\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "        ax1.set_title('Before Cap')\n",
    "        ax1.plot(data.index,column)\n",
    "        ax1.scatter(data.index[outlier_locs], outlier_values, c='r')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel(label)\n",
    "        ax1.grid()\n",
    "\n",
    "        ax2.set_title('After cap')\n",
    "        ax2.plot(data.index,capped_outlier_dataset)\n",
    "        ax2.scatter(data.index[outlier_locs], capped_outlier_dataset[outlier_locs], c='r')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel(label)\n",
    "        ax2.grid()\n",
    "        plt.savefig('capped_outliers.png')\n",
    "        plt.close()\n",
    "        \n",
    "    JetFuelPrice_visual = outliers_visualisation(JetFuelPrice_daily, 'Jet Fuel Prices'\n",
    "                                                 , JetFuelPrice_daily.Jet_Fuel_Price, 'Jet fuel price (dollar per gallon)')\n",
    "    \n",
    "######### 3.3 Data Transformation #############################################################################################\n",
    "    \n",
    "    ##### 3.3.1 Normalization #####\n",
    "    def normalize (data, column, title): #mean normalization\n",
    "        column_normalized = (column-column.mean())/column.std()\n",
    "        data_normalized = pd.DataFrame({title:column_normalized}, index = data.index)\n",
    "        return data_normalized\n",
    "\n",
    "    Close_normalized = normalize(Stock_daily, Stock_daily['Close'], 'Normalised_Close_Data')\n",
    "    AirPassengers_normalized = normalize(AirPassengers_daily, AirPassengers_daily['TOTAL'], 'Normalised_Total_Air_Passenger_Number')\n",
    "    JetFuelPrice_normalized = normalize(JetFuelPrice_Capped, JetFuelPrice_Capped['JetFuelPrice_Capped'], 'Normalised_Jet_Fuel_Price_Data')\n",
    "    \n",
    "    ##### 3.3.2 PCA dimensionality reduction #####\n",
    "    def scaler (dataset, features, label, figname):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        # Separating out the features\n",
    "        x = dataset.loc[:, features].values\n",
    "        # Standardizing the features, \n",
    "        x_std = StandardScaler().fit_transform(x)\n",
    "\n",
    "        pca = PCA().fit(x)\n",
    "        plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "        plt.title('The Quality of Dimension Reduction at Different Number of Principle Components for %s' %(label))\n",
    "        plt.xlabel('number of components')\n",
    "        plt.ylabel('cumulative explained variance')\n",
    "        plt.grid()\n",
    "        plt.savefig(figname)\n",
    "        plt.close()\n",
    "\n",
    "        return x_std\n",
    "    \n",
    "    features = ['Open', 'High', 'Low', 'Close']\n",
    "    label = 'The Stock Price'\n",
    "    figname = 'quality_of_reduction_stock_price.png'\n",
    "    stock_xstd = scaler(Stock_daily, features, label, figname)\n",
    "    \n",
    "    features = ['Total_Revenue', 'Gross_Profit', 'Operating_Income', 'Income_Before_Tax', 'EBITDA', 'Net_Income']\n",
    "    label = 'The Income Statement'\n",
    "    figname = 'quality_of_reduction_income_statement.png'\n",
    "    incomestatement_xstd = scaler(IncomeStatement_daily, features, label, figname)\n",
    "    \n",
    "    def dimension_reduction (x_std):\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        pca = PCA(n_components=1)\n",
    "        principalComponents = pca.fit_transform(x_std)\n",
    "        principalDf = pd.DataFrame(data = principalComponents, columns = ['principal_component_1'])\n",
    "        return principalDf\n",
    "    \n",
    "    # PCA dimensionality reduction for the stock price\n",
    "    stock_1D = dimension_reduction(stock_xstd)\n",
    "    stock_1D.index = Stock_daily.index\n",
    "\n",
    "    # PCA dimensionality reduction for the income statement\n",
    "    income_satement = dimension_reduction(incomestatement_xstd)\n",
    "    income_satement.index = IncomeStatement_daily.index\n",
    "    Income_Statement_Data_1D = []\n",
    "    for component in income_satement['principal_component_1']:\n",
    "        Income_Statement_Data_1D.append(float(0-component))\n",
    "\n",
    "    income_statement_1D = pd.DataFrame({'Income_Statement_Data_1D': Income_Statement_Data_1D}, index=income_satement.index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return stock_1D, income_statement_1D, Close_normalized, AirPassengers_normalized, JetFuelPrice_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fb8cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "    \n",
    "## Task 4: Data Exploration\n",
    "\n",
    "After ensuring that the data is well preprocessed, it is time to start exploring the data to carry out\n",
    "hypotheses and intuition about possible patterns that might be inferred. Depending on the data,\n",
    "different EDA (exploratory data analysis) techniques can be applied, and a large amount of\n",
    "information can be extracted.\n",
    "For example, you could do the following analysis:\n",
    "\n",
    "    \n",
    "- Time series data is normally a combination of several components:\n",
    "  - Trend represents the overall tendency of the data to increase or decrease over time.\n",
    "  - Seasonality is related to the presence of recurrent patterns that appear after regular\n",
    "intervals (like seasons).\n",
    "  - Random noise is often hard to explain and represents all those changes in the data\n",
    "that seem unexpected. Sometimes sudden changes are related to fixed or predictable\n",
    "events (i.e., public holidays).\n",
    "- Features correlation provides additional insight into the data structure. Scatter plots and\n",
    "boxplots are useful tools to spot relevant information.\n",
    "- Explain unusual behaviour.\n",
    "- Explore the correlation between stock price data and other external data that you can\n",
    "collect (as listed in Sec 2.1)\n",
    "- Use hypothesis testing to better understand the composition of your dataset and its\n",
    "representativeness.\n",
    "\n",
    "    \n",
    "At the end of this step, provide key insights on the data. This data exploration procedure should\n",
    "inform the subsequent data analysis/inference procedure, allowing one to establish a predictive\n",
    "relationship between variables.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3696927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(stock_1D, income_statement_1D, AirPassengers_normalized, JetFuelPrice_normalized):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "######### 4.1 EDA #############################################################################################################\n",
    "    \n",
    "    ##### 4.1.1 Overall Tendency\n",
    "    \n",
    "    stock_rolling_avg = stock_1D.rolling(window=180, center=True).mean()\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    plt.plot(stock_rolling_avg.index, stock_rolling_avg['principal_component_1'])\n",
    "    plt.title('180 Days Rolling Averaged American Airlines Group Inc. Stock Price', fontsize=15)\n",
    "    plt.xlabel('Date', fontsize=15)\n",
    "    plt.ylabel('American Airlines Group Inc. Stock Price', fontsize=15)\n",
    "    plt.savefig('Tendency_stock_prices.png')\n",
    "    plt.close()\n",
    "    \n",
    "    ##### 4.1.1 Seasonality\n",
    "    from calendar import month_abbr\n",
    "    \n",
    "    seasonal_cycle = stock_1D.rolling(window=30, center=True).mean().groupby(stock_1D.index.dayofyear).mean()\n",
    "    q25 = stock_1D.rolling(window=30, center=True).mean().groupby(stock_1D.index.dayofyear).quantile(0.25)\n",
    "    q75 = stock_1D.rolling(window=30, center=True).mean().groupby(stock_1D.index.dayofyear).quantile(0.75)\n",
    "    \n",
    "    month_ticks = month_abbr[1:]\n",
    "    plt.figure(figsize=(16, 7))\n",
    "    plt.plot(seasonal_cycle.index, seasonal_cycle.principal_component_1, color='b')\n",
    "    plt.fill_between(seasonal_cycle.index, q25.values.ravel(), q75.values.ravel(), color='b', alpha=0.3)\n",
    "    plt.xticks(np.linspace(0,365,13)[:-1], (month_ticks))\n",
    "    plt.grid(ls=':')\n",
    "    plt.xlabel('Month', fontsize=15)\n",
    "    plt.ylabel('American Airlines Group Inc. Stock Price', fontsize=15);\n",
    "    plt.xlim(0, 365)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.title('Yearly Seasonality of 30 Days Rolling Averaged Stock Price', fontsize=15)\n",
    "    plt.savefig('seasonality_stock_prices.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Explore dependency on year and month via carpet plot/heatmap\n",
    "    Seasonality = stock_1D.copy()\n",
    "    Seasonality['Year'] = Seasonality.index.year\n",
    "    Seasonality['Quarter'] = Seasonality.index.quarter\n",
    "    Seasonality['Month'] = Seasonality.index.month\n",
    "    Seasonality['Day_of_Week'] = Seasonality.index.day_name()\n",
    "\n",
    "    def seasonality_heatmap(size, xlabel, ylabel, figname):\n",
    "        data = Seasonality.groupby([ylabel,xlabel]).mean().unstack().principal_component_1\n",
    "\n",
    "        f, ax = plt.subplots(figsize=(12,6))\n",
    "        sns.heatmap(data, ax=ax, cmap=plt.cm.viridis, cbar_kws={'boundaries':size})\n",
    "        cbax = f.axes[1]\n",
    "        [l.set_fontsize(13) for l in cbax.yaxis.get_ticklabels()]\n",
    "        cbax.set_ylabel('Dimensionality Reduced AAL Stock Prices', fontsize=13)\n",
    "        [ax.axhline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 7)]\n",
    "        [ax.axvline(x, ls=':', lw=0.5, color='0.8') for x in np.arange(1, 24)];\n",
    "        ax.set_title('Dimensionality Reduced AAL Stock Prices per %s and %s'%(ylabel, xlabel), fontsize=16)\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "        ax.set_xlabel(xlabel, fontsize=15)\n",
    "        ax.set_ylabel(ylabel, fontsize=15)\n",
    "        plt.savefig(figname)\n",
    "        plt.close()\n",
    "    \n",
    "    seasonality_heatmap(np.arange(-3,3.8,0.4), 'Month', 'Year', 'month_year_seasonality_heatmap')\n",
    "    seasonality_heatmap(np.arange(-0.5,0.5,0.1), 'Month', 'Day_of_Week', 'day_month_seasonality_heatmap')\n",
    "    \n",
    "    ##### 4.1.2 Correlation stock price data and other external data #####\n",
    "    def correlation (column1, column2, title, xlabel, ylabel, scatterplot, heatmap):\n",
    "        x = column1\n",
    "        y = column2\n",
    "\n",
    "        # Scatter plot\n",
    "        plt.scatter(x,y)\n",
    "        plt.title('Correlation between %s'%(title))\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.savefig(scatterplot)\n",
    "        plt.close()\n",
    "\n",
    "        # Covariance\n",
    "        Covariance = np.cov(x, y)\n",
    "        print(\"Covariance Matrix of %s is:\\n\"%(title), Covariance)\n",
    "\n",
    "        # Covariance heatmap\n",
    "        import seaborn as sn\n",
    "        sn.heatmap(Covariance, annot=True, fmt='g')\n",
    "        plt.savefig(heatmap)\n",
    "        plt.close()\n",
    "\n",
    "        # Correlation\n",
    "        from scipy.stats import pearsonr\n",
    "        corr, _ = pearsonr(x, y)\n",
    "        print('Correlation between %s is:\\n%f'%(title, corr))\n",
    "    \n",
    "    # Stock price and income statement price\n",
    "    title = 'AAL Stock Price and Income Statement'\n",
    "    xlabel = 'Dimensionality Reduced AAL Stock Price'\n",
    "    ylabel = 'Dimensionality Reduced AAL Income Statement'\n",
    "    scatterplot = 'stock_income_statement_scatterplot.png'\n",
    "    heatmap = 'stock_income_statement_heatmap.png'\n",
    "    correlation(stock_1D['principal_component_1'], income_statement_1D['Income_Statement_Data_1D']\n",
    "                , title, xlabel, ylabel, scatterplot, heatmap)\n",
    "    \n",
    "    # Stock price and air passengers\n",
    "    title = 'Correlation between AAL Stock Prices and Air Passengers'\n",
    "    xlabel = 'Dimensionality Reduced AAL Stock Price'\n",
    "    ylabel = 'Normalized Air Passenger Number'\n",
    "    scatterplot = 'stock_air_passenger_scatterplot.png'\n",
    "    heatmap = 'stock_air_passenger_heatmap.png'\n",
    "    correlation(stock_1D['principal_component_1'], AirPassengers_normalized['Normalised_Total_Air_Passenger_Number']\n",
    "                , title, xlabel, ylabel, scatterplot, heatmap)\n",
    "    \n",
    "    # Stock price and jet fuel price\n",
    "    title = 'Correlation between AAL Stock Prices and Jet Fuel Price'\n",
    "    xlabel = 'Dimensionality Reduced AAL Stock Price'\n",
    "    ylabel = 'Normalized Jet Fuel Price'\n",
    "    scatterplot = 'stock_jet_fuel_scatterplot.png'\n",
    "    heatmap = 'stock_jet_fuel_heatmap.png'\n",
    "    correlation(stock_1D['principal_component_1'], JetFuelPrice_normalized['Normalised_Jet_Fuel_Price_Data']\n",
    "                , title, xlabel, ylabel, scatterplot, heatmap)\n",
    "    \n",
    "######### 4.2 Hypothesis Test #################################################################################################\n",
    "    \n",
    "    stock_trend = stock_1D.copy()\n",
    "    jetfuel_trend = JetFuelPrice_normalized.copy()\n",
    "    stock_trend['Daily_Perc_Change'] = stock_1D['principal_component_1'].pct_change().fillna(value=0)*100\n",
    "    jetfuel_trend['Daily_Perc_Change'] = JetFuelPrice_normalized['Normalised_Jet_Fuel_Price_Data'].pct_change().fillna(value=0)*100\n",
    "\n",
    "    Trend = pd.DataFrame(index = stock_1D.index)\n",
    "    Trend['Stock_Change'] = stock_trend['Daily_Perc_Change']\n",
    "    Trend['JetFuelPrice_Change'] = jetfuel_trend['Daily_Perc_Change']\n",
    "    \n",
    "    stock_rise_fuel_rise = len(Trend[(Trend['Stock_Change']>0) & (Trend['JetFuelPrice_Change']>0)])\n",
    "    stock_drop_fuel_rise = len(Trend[(Trend['Stock_Change']<0) & (Trend['JetFuelPrice_Change']>0)])\n",
    "    stock_rise_fuel_drop = len(Trend[(Trend['Stock_Change']>0) & (Trend['JetFuelPrice_Change']<0)])\n",
    "    stock_drop_fuel_drop = len(Trend[(Trend['Stock_Change']<0) & (Trend['JetFuelPrice_Change']<0)])\n",
    "    \n",
    "    fuel_rise = [stock_rise_fuel_rise, stock_drop_fuel_rise]\n",
    "    fuel_drop = [stock_rise_fuel_drop, stock_drop_fuel_drop]\n",
    "    \n",
    "    index = ['Stock_Price_Rise', 'Stock_Price_Drop']\n",
    "    d = {'Jet_Fuel_Price_Rise':fuel_rise, 'Jet_Fuel_Price_Drop':fuel_drop}\n",
    "    Table_Simple=pd.DataFrame(data=d,index=index)\n",
    "    print(Table_Simple)\n",
    "    \n",
    "    stock_rise_total = stock_rise_fuel_rise + stock_rise_fuel_drop\n",
    "    stock_drop_total = stock_drop_fuel_rise + stock_drop_fuel_drop\n",
    "    stock_total = stock_rise_total + stock_drop_total\n",
    "    fuel_rise_total = stock_rise_fuel_rise + stock_drop_fuel_rise\n",
    "    fuel_drop_total = stock_rise_fuel_drop + stock_drop_fuel_drop\n",
    "    \n",
    "    fuel_rise = [stock_rise_fuel_rise, stock_drop_fuel_rise, fuel_rise_total]\n",
    "    fuel_drop = [stock_rise_fuel_drop, stock_drop_fuel_drop, fuel_drop_total]\n",
    "    total = [stock_rise_total, stock_drop_total, stock_total]\n",
    "    \n",
    "    index = ['Stock_Price_Rise', 'Stock_Price_Drop', 'Total']\n",
    "    d = {'Jet_Fuel_Price_Rise':fuel_rise, 'Jet_Fuel_Price_Drop':fuel_drop, 'Total': total}\n",
    "    Table_with_Total=pd.DataFrame(data=d,index=index)\n",
    "    \n",
    "    from scipy.stats import chi2_contingency\n",
    "    from scipy.stats import chi2\n",
    "\n",
    "    stat, p, dof, expected = chi2_contingency(Table_Simple)\n",
    "    print(\"statistic\",stat)\n",
    "    print(\"p-value\",p)\n",
    "    print(\"degres of fredom: \",dof)\n",
    "    print(\"table of expected frequencies\\n\",expected)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    if abs(stat) >= critical:\n",
    "        print('Dependent (reject H0)')\n",
    "    else:\n",
    "        print('Independent (fail to reject H0)')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3e459",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-info\">\n",
    "\n",
    "## Task 5: Inference\n",
    "\n",
    "Train a model to predict the closing stock price on each day for the data you have already\n",
    "collected, stored, preprocessed and explored from previous steps. The data must be spanning\n",
    "from April 2017 to April 202 1.\n",
    "You should develop two separate models:\n",
    "\n",
    "\n",
    "1. A model for predicting the closing stock price on each day for a 1-month time window (until\n",
    "    end of May 202 1 ), using only time series of stock prices.\n",
    "2. A model for predicting the closing stock price on each day for a 1-month time window (until\n",
    "    end of May 202 1 ), using the time series of stock prices and the auxiliary data you collected.\n",
    "Which model is performing better? How do you measure performance and why? How could you\n",
    "further improve the performance? Are the models capable of predicting the closing stock prices\n",
    "far into the future?\n",
    "\n",
    "[IMPORTANT NOTE] For these tasks, you are not expected to compare model architectures, but\n",
    "examine and analyse the differences when training the same model with multiple data attributes\n",
    "and information from sources. Therefore, you should decide a single model suitable for time series\n",
    "data to solve the tasks described above. Please see the lecture slides for tips on model selection\n",
    "and feel free to experiment before selecting one.\n",
    "\n",
    "The following would help you evaluate your approach and highlight potential weaknesses in your\n",
    "process:\n",
    "\n",
    "1. Evaluate the performance of your model using different metrics, e.g. mean squared error,\n",
    "    mean absolute error or R-squared.\n",
    "2. Use ARIMA and Facebook Prophet to explore the uncertainty on your model’s predicted\n",
    "    values by employing confidence bands.\n",
    "3. Result visualization: create joint plots showing marginal distributions to understand the\n",
    "    correlation between actual and predicted values.\n",
    "4. Finding the mean, median and skewness of the residual distribution might provide\n",
    "    additional insight into the predictive capability of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3344d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(data, target_feature, auxiliary_data1, auxiliar_data2):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from fbprophet import Prophet\n",
    "    \n",
    "    ##### Model with the stock price dataset #####\n",
    "    \n",
    "    df_stock = data.copy()\n",
    "    df_stock.reset_index(inplace=True)\n",
    "    df_stock = df_stock.rename({'Date':'ds', '{}'.format(target_feature):'y'}, axis=1)\n",
    "    #df_new = prepare_data(Close_normalized, 'Normalised_Close_Data')\n",
    "    \n",
    "    # Holidays Packdge\n",
    "    import holidays\n",
    "    holidays_df = pd.DataFrame([], columns = ['ds','holiday'])\n",
    "    ldates = []\n",
    "    lnames = []\n",
    "    for date, name in sorted(holidays.UnitedStates(years=np.arange(2011, 2020 + 1)).items()):\n",
    "        ldates.append(date)\n",
    "        lnames.append(name)\n",
    "\n",
    "    ldates = np.array(ldates)\n",
    "    lnames = np.array(lnames)\n",
    "    holidays_df.loc[:,'ds'] = ldates\n",
    "    holidays_df.loc[:,'holiday'] = lnames\n",
    "    holidays_df.holiday.unique()\n",
    "    \n",
    "    holidays_df.loc[:,'holiday'] = holidays_df.loc[:,'holiday'].apply(lambda x : x.replace(' (Observed)',''))\n",
    "    \n",
    "    # Train test split\n",
    "    train_stock = df_stock.set_index('ds').loc[:'2021-04-30', :].reset_index()\n",
    "    test_stock = df_stock.set_index('ds').loc['2021-05-01':'2021-05-31', :].reset_index()\n",
    "    \n",
    "    # model fit\n",
    "    model_stock = Prophet(holidays=holidays_df,\n",
    "                seasonality_mode='multiplicative',\n",
    "                yearly_seasonality=True, \n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False)\n",
    "    model_stock.fit(train_stock)\n",
    "    \n",
    "    future_stock = model_stock.make_future_dataframe(periods=31, freq='1D')\n",
    "    \n",
    "    \n",
    "    ##### Model with the stock price dataset and auxiliary datasets #####\n",
    "    \n",
    "    IncomeStatement_regressor = auxiliary_data1[(auxiliary_data1.index >= '2017-04-03') &\n",
    "                        (auxiliary_data1.index <= '2021-05-31')].sort_index(axis=0,ascending=True)\n",
    "    AirPassenger_regressor = auxiliar_data2[(auxiliar_data2.index >= '2017-04-03') &\n",
    "                        (auxiliar_data2.index <= '2021-05-31')].sort_index(axis=0,ascending=True)\n",
    "    \n",
    "    df_regressors = pd.concat([df_stock, IncomeStatement_regressor.loc[:, ['Income_Statement_Data_1D']].reset_index(drop=True)], axis=1)\n",
    "    df_regressors = pd.concat([df_regressors, AirPassenger_regressor.loc[:, ['Normalised_Total_Air_Passenger_Number']].reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    def is_pandemic_affected(ds):\n",
    "        date = pd.to_datetime(ds)\n",
    "        return date.year == 2020\n",
    "    \n",
    "    df_regressors['pandemic_affected'] = df_regressors['ds'].apply(is_pandemic_affected)\n",
    "    df_regressors['not_pandemic_affected'] = ~df_regressors['ds'].apply(is_pandemic_affected)\n",
    "    \n",
    "    train_regressors = df_regressors.set_index('ds').loc[:'2021-04-30', :].reset_index()\n",
    "    test_regressors = df_regressors.set_index('ds').loc['2021-05-01':'2021-05-31', :].reset_index()\n",
    "    \n",
    "    model_regressors = Prophet(holidays=holidays_df, \n",
    "                                    seasonality_mode='multiplicative',\n",
    "                                    yearly_seasonality=True, \n",
    "                                    weekly_seasonality=True,\n",
    "                                    daily_seasonality=False)\n",
    "    \n",
    "    model_regressors.add_regressor('Income_Statement_Data_1D', mode='multiplicative')\n",
    "    model_regressors.add_regressor('Normalised_Total_Air_Passenger_Number', mode='multiplicative')\n",
    "    model_regressors.add_seasonality(name='pandemic_affected', period=365,\n",
    "                                          fourier_order=3, mode='multiplicative', condition_name='pandemic_affected')\n",
    "    model_regressors.add_seasonality(name='not_pandemic_affected', period=365,\n",
    "                                          fourier_order=3, mode='multiplicative', condition_name='not_pandemic_affected')\n",
    "    model_regressors.fit(train_regressors)\n",
    "    \n",
    "    future_regressors = pd.concat([future_stock, IncomeStatement_regressor.loc[:, ['Income_Statement_Data_1D']].reset_index(drop=True)], axis=1)\n",
    "    future_regressors = pd.concat([future_regressors, AirPassenger_regressor.loc[:, ['Normalised_Total_Air_Passenger_Number']].reset_index(drop=True)], axis=1)\n",
    "    future_regressors['pandemic_affected'] = future_regressors['ds'].apply(is_pandemic_affected)\n",
    "    future_regressors['not_pandemic_affected'] = ~future_regressors['ds'].apply(is_pandemic_affected)\n",
    "    \n",
    "    return model_stock, train_stock, test_stock, future_stock, model_regressors, train_regressors, test_regressors, future_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d69f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, future, figname):\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    #Function to convert the output Prophet dataframe to a datetime index and append the actual target values at the end\n",
    "    forecast.index = pd.to_datetime(forecast.ds)\n",
    "    train_data.index = pd.to_datetime(train_data.ds)\n",
    "    test_data.index = pd.to_datetime(test_data.ds)\n",
    "    data = pd.concat([train_data, test_data], axis=0)\n",
    "    forecast.loc[:,'y'] = data.loc[:,'y']\n",
    "\n",
    "    #plot the predictions\n",
    "    f, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    train = forecast.loc['2020-01-01':'2021-04-30',:]\n",
    "    ax.plot(train.index, train.y, 'ko', markersize=3)\n",
    "    ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n",
    "    ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n",
    "\n",
    "    test = forecast.loc['2021-05-01':'2021-05-31',:]\n",
    "    ax.plot(test.index, test.y, 'ro', markersize=3)\n",
    "    ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n",
    "    ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n",
    "    ax.axvline(forecast.loc['2021-05-01', 'ds'], color='k', ls='--', alpha=0.7)\n",
    "    ax.grid(ls=':', lw=0.5)\n",
    "    plt.savefig(figname)\n",
    "    plt.close()\n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c59fa762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_data, modelname):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    x = val_data.loc[:'2021-04-30', :]\n",
    "    x_true = x['y']\n",
    "    x_pred = x['yhat']\n",
    "    \n",
    "    y = val_data.loc['2021-05-01':'2021-05-31', :]\n",
    "    y_true = y['y']\n",
    "    y_pred = y['yhat']\n",
    "    \n",
    "    # mean absolute error\n",
    "    print('The MAE of the Train Set of %s is:\\n%f'%(modelname, mean_absolute_error(x_true, x_pred)))\n",
    "    print('The MAE of the Test Set of %s is:\\n%f'%(modelname, mean_absolute_error(y_true, y_pred)))\n",
    "    \n",
    "    # mean squared error\n",
    "    print('The MSE of the Train Set of %s is:\\n%f'%(modelname, mean_squared_error(x_true, x_pred)))\n",
    "    print('The MSE of the Test Set of %s is:\\n%f'%(modelname, mean_squared_error(y_true, y_pred)))\n",
    "    \n",
    "    def create_joint_plot(forecast, title, x, y, figname): \n",
    "        g = sns.jointplot(x=forecast['yhat'], y=forecast['y'], kind=\"reg\", color=\"b\")\n",
    "        g.fig.set_figwidth(8)\n",
    "        g.fig.set_figheight(8)\n",
    "\n",
    "        ax = g.fig.axes[1]\n",
    "        if title is not None: \n",
    "            ax.set_title(title, fontsize=16)\n",
    "\n",
    "        ax = g.fig.axes[0]\n",
    "        ax.text(x, y, \"R = {:+4.2f}\".format(forecast.loc[:,['y','yhat']].corr().iloc[0,1]), fontsize=16)\n",
    "        ax.set_xlabel('Predictions', fontsize=15)\n",
    "        ax.set_ylabel('Observations', fontsize=15)\n",
    "        ax.grid(ls=':')\n",
    "        [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "        [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()];\n",
    "        ax.grid(ls=':')\n",
    "        plt.savefig(figname)\n",
    "        plt.close()\n",
    "        \n",
    "    title ='Join Plot of the Train Set of %s'%(modelname)\n",
    "    fig = r'Train Set of %s.png'%(modelname)\n",
    "    create_joint_plot(val_data.loc[:'2021-04-30', :], title, -1, 1.5, fig)\n",
    "    \n",
    "    title ='Join Plot of the Test Set of %s'%(modelname)\n",
    "    fig = r'Test Set of %s.png'%(modelname)\n",
    "    create_joint_plot(val_data.loc['2021-05-01':'2021-05-31', :], title, 0.56, -0.5, fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75246e0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\">\n",
    "\n",
    "## Autorun\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f22c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    ########## Task 1: Data Acquisition ###########\n",
    "    \n",
    "    aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price = acquire()\n",
    "\n",
    "    ########## Task 2: Data Storage ###########\n",
    "\n",
    "    store(aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price)\n",
    "    \n",
    "    # Retrieve data from api\n",
    "    stock_parsed = retrieve(\"StockPrice\")\n",
    "    IncomeStatement_parsed = retrieve(\"IncomeStatement\")\n",
    "    AirPassengers_parsed = retrieve(\"AirPassenger\")\n",
    "    JetFuelPrice_parsed = retrieve(\"JetFuelPrice\")\n",
    "    \n",
    "    ########## Task 3: Data Preprocessing ###########\n",
    "    \n",
    "    stock_1D, income_statement_1D, Close_normalized, AirPassengers_normalized, JetFuelPrice_normalized = process(\n",
    "        aal_stock, IncomeStatement_raw, AirPassengers_raw, JetFuel_Price)\n",
    "    \n",
    "    ########## Task 4: Data Exploration ###########\n",
    "    \n",
    "    explore(stock_1D, income_statement_1D, AirPassengers_normalized, JetFuelPrice_normalized)\n",
    "    \n",
    "    ########## Task 5: Inference ###########\n",
    "\n",
    "    model_stock, train_stock, test_stock, future_stock, model_regressors, train_regressors, test_regressors, future_regressors = create_models(\n",
    "        Close_normalized, 'Normalised_Close_Data', income_statement_1D, AirPassengers_normalized)\n",
    "    \n",
    "    model_stock_result = train(model_stock, train_stock, test_stock, future_stock, 'prediction_result_stock.png')\n",
    "    model_with_regressors_result = train(model_regressors, train_regressors, test_regressors, future_regressors, 'prediction_result_auxiliarydata.png')\n",
    "    \n",
    "    model_stock_evaluation = evaluate(model_stock_result, 'the Model with Stock Price data Only')\n",
    "    model_with_regressors_evaluation = evaluate(model_with_regressors_result, 'the Model with Stock Price and Auxiliary Data')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99cc427f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values detected in Stock Price is \n",
      " Open            0\n",
      "High            0\n",
      "Low             0\n",
      "Close           0\n",
      "Volume          0\n",
      "Dividends       0\n",
      "Stock Splits    0\n",
      "dtype: int64\n",
      "Number of missing values detected in Income Statement is \n",
      " Total_Revenue        0\n",
      "Gross_Profit         0\n",
      "Operating_Income     0\n",
      "Income_Before_Tax    0\n",
      "EBITDA               0\n",
      "Net_Income           0\n",
      "dtype: int64\n",
      "Number of missing values detected in Air Passengers is \n",
      " DOMESTIC         0\n",
      "INTERNATIONAL    0\n",
      "TOTAL            0\n",
      "dtype: int64\n",
      "Number of missing values detected in Jet Fuel Price is \n",
      " Jet_Fuel_Price    0\n",
      "dtype: int64\n",
      "The open stock price has no outliers.\n",
      "The high stock price has no outliers.\n",
      "The low stock price has no outliers.\n",
      "The close stock price has no outliers.\n",
      "The total Revenue has no outliers.\n",
      "The gross profit has no outliers.\n",
      "The operating income has no outliers.\n",
      "The income before tax has no outliers.\n",
      "The ebitda has no outliers.\n",
      "The net income has no outliers.\n",
      "The air passengers has no outliers.\n",
      "Outliers exist in jet fuel price is: ['2020-03-20T00:00:00.000000000' '2020-03-21T00:00:00.000000000'\n",
      " '2020-03-22T00:00:00.000000000' '2020-03-23T00:00:00.000000000'\n",
      " '2020-03-24T00:00:00.000000000' '2020-03-25T00:00:00.000000000'\n",
      " '2020-03-31T00:00:00.000000000' '2020-04-01T00:00:00.000000000'\n",
      " '2020-04-02T00:00:00.000000000' '2020-04-09T00:00:00.000000000'\n",
      " '2020-04-10T00:00:00.000000000' '2020-04-11T00:00:00.000000000'\n",
      " '2020-04-12T00:00:00.000000000' '2020-04-13T00:00:00.000000000'\n",
      " '2020-04-14T00:00:00.000000000' '2020-04-15T00:00:00.000000000'\n",
      " '2020-04-16T00:00:00.000000000' '2020-04-17T00:00:00.000000000'\n",
      " '2020-04-18T00:00:00.000000000' '2020-04-19T00:00:00.000000000'\n",
      " '2020-04-20T00:00:00.000000000' '2020-04-21T00:00:00.000000000'\n",
      " '2020-04-22T00:00:00.000000000' '2020-04-23T00:00:00.000000000'\n",
      " '2020-04-24T00:00:00.000000000' '2020-04-25T00:00:00.000000000'\n",
      " '2020-04-26T00:00:00.000000000' '2020-04-27T00:00:00.000000000'\n",
      " '2020-04-28T00:00:00.000000000' '2020-04-29T00:00:00.000000000'\n",
      " '2020-04-30T00:00:00.000000000' '2020-05-01T00:00:00.000000000'\n",
      " '2020-05-02T00:00:00.000000000' '2020-05-03T00:00:00.000000000'\n",
      " '2020-05-04T00:00:00.000000000' '2020-05-05T00:00:00.000000000'\n",
      " '2020-05-06T00:00:00.000000000' '2020-05-07T00:00:00.000000000'\n",
      " '2020-05-08T00:00:00.000000000' '2020-05-09T00:00:00.000000000'\n",
      " '2020-05-10T00:00:00.000000000' '2020-05-11T00:00:00.000000000'\n",
      " '2020-05-12T00:00:00.000000000' '2020-05-13T00:00:00.000000000'\n",
      " '2020-05-14T00:00:00.000000000']\n",
      "The corresponding outliers are:  [0.716      0.70966667 0.70333333 0.697      0.687      0.692\n",
      " 0.697      0.65       0.705      0.69       0.69425    0.6985\n",
      " 0.70275    0.707      0.662      0.625      0.653      0.667\n",
      " 0.64466667 0.62233333 0.6        0.453      0.507      0.497\n",
      " 0.453      0.43766667 0.42233333 0.407      0.416      0.485\n",
      " 0.517      0.48       0.48766667 0.49533333 0.503      0.574\n",
      " 0.504      0.518      0.579      0.571      0.563      0.555\n",
      " 0.629      0.637      0.703     ]\n",
      "Covariance Matrix of AAL Stock Price and Income Statement is:\n",
      " [[4.00004026 3.69877433]\n",
      " [3.69877433 5.3648561 ]]\n",
      "Correlation between AAL Stock Price and Income Statement is:\n",
      "0.798448\n",
      "Covariance Matrix of Correlation between AAL Stock Prices and Air Passengers is:\n",
      " [[4.00004026 1.51010027]\n",
      " [1.51010027 1.        ]]\n",
      "Correlation between Correlation between AAL Stock Prices and Air Passengers is:\n",
      "0.755046\n",
      "Covariance Matrix of Correlation between AAL Stock Prices and Jet Fuel Price is:\n",
      " [[4.00004026 1.11014148]\n",
      " [1.11014148 1.        ]]\n",
      "Correlation between Correlation between AAL Stock Prices and Jet Fuel Price is:\n",
      "0.555068\n",
      "                  Jet_Fuel_Price_Rise  Jet_Fuel_Price_Drop\n",
      "Stock_Price_Rise                  389                  423\n",
      "Stock_Price_Drop                  357                  404\n",
      "statistic 0.11847594179019572\n",
      "p-value 0.7306933678907452\n",
      "degres of fredom:  1\n",
      "table of expected frequencies\n",
      " [[385.093452 426.906548]\n",
      " [360.906548 400.093452]]\n",
      "Independent (fail to reject H0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE of the Train Set of the Model with Stock Price data Only is:\n",
      "0.161159\n",
      "The MAE of the Test Set of the Model with Stock Price data Only is:\n",
      "0.128322\n",
      "The MSE of the Train Set of the Model with Stock Price data Only is:\n",
      "0.038276\n",
      "The MSE of the Test Set of the Model with Stock Price data Only is:\n",
      "0.019576\n",
      "The MAE of the Train Set of the Model with Stock Price and Auxiliary Data is:\n",
      "0.116225\n",
      "The MAE of the Test Set of the Model with Stock Price and Auxiliary Data is:\n",
      "0.061159\n",
      "The MSE of the Train Set of the Model with Stock Price and Auxiliary Data is:\n",
      "0.022571\n",
      "The MSE of the Test Set of the Model with Stock Price and Auxiliary Data is:\n",
      "0.005399\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076866a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:daps] *",
   "language": "python",
   "name": "conda-env-daps-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
